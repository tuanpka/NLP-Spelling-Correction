{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "sT1yT1clAuAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40f957b6-1d2b-4420-faf7-d0accc240ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJWJInFN8pmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7dae39-b223-4d64-db4e-88d9dccf186c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length:  1928\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class FileData(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        with open(path, encoding='utf-16') as f:\n",
        "          self.data = f.read()\n",
        "          #print(self.data)\n",
        "\n",
        "ABSOLUTE_PATH = r\"/content/drive/MyDrive/vnese-spell-correction-lstm/Train_Full\"\n",
        "\n",
        "c_tri =  \"/Chinh tri Xa hoi\"\n",
        "\n",
        "doi_song = \"/Doi song\"\n",
        "\n",
        "khoa_hoc = \"/Khoa hoc\"\n",
        "\n",
        "kinh_doanh = \"/Kinh doanh\"\n",
        "\n",
        "p_luat = \"/Phap luat\"\n",
        "\n",
        "suc_khoe = \"/Suc khoe\"\n",
        "\n",
        "the_gioi = \"/The gioi\"\n",
        "\n",
        "the_thao = \"/The thao\"\n",
        "\n",
        "van_hoa = \"/Van hoa\"\n",
        "\n",
        "vi_tinh = \"/Vi tinh\"\n",
        "corpus = [c_tri, doi_song, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_gioi, the_thao, van_hoa, vi_tinh]\n",
        "\n",
        "#Extract folder path\n",
        "for folder_path in range(len(corpus)):\n",
        "    corpus[folder_path] = ABSOLUTE_PATH + corpus[folder_path]\n",
        "\n",
        "file_list = []\n",
        "\n",
        "#Extracting text from corpus\n",
        "for folder_path in corpus:\n",
        "    count = 0\n",
        "    for name in os.listdir(folder_path):\n",
        "        count +=1\n",
        "        if count == 1500:\n",
        "          break\n",
        "        path = os.path.join(folder_path, name)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "        file = FileData(path)\n",
        "        file_list.append( file.data )\n",
        "\n",
        "print('Corpus length: ', len(file_list))\n",
        "\n",
        "#Save extracted corpus as Pickle file\n",
        "path_corpus = r\"/content/drive/MyDrive/vnese-spell-correction-lstm/train_corpus.pkl\"\n",
        "\n",
        "with open(path_corpus, 'wb') as pickle_file:\n",
        "    pickle.dump(file_list, pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybOztiQEJyAr",
        "outputId": "6f34483a-4084-4166-c312-27cda576c5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "from unidecode import unidecode\n",
        "import itertools\n",
        "from nltk import ngrams\n",
        "from tqdm import tqdm\n",
        "\n",
        "path_corpus = r\"/content/drive/MyDrive/vnese-spell-correction-lstm/train_corpus.pkl\"\n",
        "\n",
        "with open(path_corpus, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'\n",
        "\n",
        "#Extracting sentence from corpus\n",
        "def latin_extract(data):\n",
        "\n",
        "    # extract Latin- characters only\n",
        "    latin_extract_data=[]\n",
        "    # duyet qua tung van ban\n",
        "    for i in data:\n",
        "      if i == 1:\n",
        "        break\n",
        "      # thay the xuong dong la dau cham ket thuc\n",
        "      i=i.replace(\"\\n\",\".\")\n",
        "      # tach van ban theo dau cham ket thuc\n",
        "      sentences=i.split(\".\")\n",
        "      for j in sentences:\n",
        "          if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n",
        "\n",
        "              latin_extract_data.append(j)\n",
        "\n",
        "    return latin_extract_data\n",
        "\n",
        "training_data = latin_extract(data)\n",
        "i = 100\n",
        "#Listing all typos, regional dialects\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n",
        "\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n",
        "\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n",
        "\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n",
        "\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n",
        "\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n",
        "\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n",
        "\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n",
        "\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n",
        "\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n",
        "\n",
        "# dia phuong\n",
        "region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n",
        "region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n",
        "\n",
        "# nguyen am\n",
        "vowel=list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\")\n",
        "\n",
        "# viet tat\n",
        "acronym={\"không\":\"ko\",\" anh\":\" a\",\"em\":\"e\",\"biết\":\"bít\",\"giờ\":\"h\",\"gì\":\"j\",\"muốn\":\"mún\",\"học\":\"hok\",\"yêu\":\"iu\",\n",
        "         \"chồng\":\"ck\",\"vợ\":\"vk\",\" ông\":\" ô\",\"được\":\"đc\",\"tôi\":\"t\",\n",
        "         \"Không\":\"Ko\",\" Anh\":\" A\",\"Em\":\"E\",\"Biết\":\"Bít\",\"Giờ\":\"H\",\"Gì\":\"J\",\"Muốn\":\"Mún\",\"Học\":\"Hok\",\"Yêu\":\"Iu\",\n",
        "         \"Chồng\":\"Ck\",\"Vợ\":\"Vk\",\" Ông\":\" Ô\",\"Được\":\"Đc\",\"Tôi\":\"T\",}\n",
        "\n",
        "# teencode\n",
        "teen={\"ch\":\"ck\",\"ph\":\"f\",\"th\":\"tk\",\"nh\":\"nk\",\n",
        "      \"Ch\":\"Ck\",\"Ph\":\"F\",\"Th\":\"Tk\",\"Nh\":\"Nk\"}\n",
        "\n",
        "# function for adding mistake( noise)\n",
        "def teen_code(sentence,pivot):\n",
        "    random = np.random.uniform(0,1,1)[0]\n",
        "    new_sentence=str(sentence)\n",
        "    if random>pivot:\n",
        "        for word in acronym.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random2 = np.random.uniform(0,1,1)[0]\n",
        "                if random2 <0.5:\n",
        "                    new_sentence=new_sentence.replace(word,acronym[word])\n",
        "        for word in teen.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random3 = np.random.uniform(0,1,1)[0]\n",
        "                if random3 <0.05:\n",
        "                    new_sentence=new_sentence.replace(word,teen[word])\n",
        "        return new_sentence\n",
        "    else:\n",
        "        return sentence\n",
        "\n",
        "\n",
        "def add_noise(sentence, pivot1,pivot2):\n",
        "    sentence=teen_code(sentence,0.5)\n",
        "    noisy_sentence = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        if sentence[i] not in letters:\n",
        "            noisy_sentence+=sentence[i]\n",
        "        else:\n",
        "            random = np.random.uniform(0,1,1)[0]\n",
        "            if random < pivot1:\n",
        "                noisy_sentence+=(sentence[i])\n",
        "            elif random<pivot2:\n",
        "                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n",
        "                    random2=np.random.uniform(0,1,1)[0]\n",
        "                    if random2<=0.4:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random2<0.8:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random2<0.95 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in typo.keys():\n",
        "                    random3=np.random.uniform(0,1,1)[0]\n",
        "                    if random3<=0.6:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random3<0.9 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in region.keys():\n",
        "                    random4=np.random.uniform(0,1,1)[0]\n",
        "                    if random4<=0.6:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random4<0.85 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif i<len(sentence)-1 :\n",
        "                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n",
        "                        random5=np.random.uniform(0,1,1)[0]\n",
        "                        if random5<=0.9:\n",
        "                            noisy_sentence+=region2[sentence[i]]\n",
        "                        else:\n",
        "                            noisy_sentence+=sentence[i]\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "\n",
        "            else:\n",
        "                new_random = np.random.uniform(0,1,1)[0]\n",
        "                if new_random <=0.33:\n",
        "                    if i == (len(sentence) - 1):\n",
        "                        continue\n",
        "                    else:\n",
        "                        noisy_sentence+=(sentence[i+1])\n",
        "                        noisy_sentence+=(sentence[i])\n",
        "                        i += 1\n",
        "                elif new_random <= 0.66:\n",
        "                    random_letter = np.random.choice(letters2, 1)[0]\n",
        "                    noisy_sentence+=random_letter\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "        i += 1\n",
        "    return noisy_sentence\n",
        "\n",
        "def extract_phrases(text):\n",
        "    return re.findall(r'\\w[\\w ]+', text)\n",
        "\n",
        "def _extract_phrases(data):\n",
        "    phrases = itertools.chain.from_iterable(extract_phrases(text) for text in data)\n",
        "    phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n",
        "\n",
        "    return phrases\n",
        "\n",
        "phrases = _extract_phrases(training_data)\n",
        "\n",
        "#Generate Bi-gram\n",
        "\n",
        "#A Vietnamese word do not contain more than 7 characters, so an bi-gram do not have more than 15 characters\n",
        "NGRAM = 2\n",
        "MAXLEN = 40\n",
        "\n",
        "def gen_ngrams(words, n=2):\n",
        "    return ngrams(words.split(), n)\n",
        "#Mục đích: Tạo danh sách các bi-grams từ danh sách các cụm từ (phrases) và lọc ra các bi-grams thỏa mãn các điều kiện.\n",
        "def generate_bi_grams(phrases):\n",
        "    list_ngrams = []\n",
        "    for p in tqdm(phrases):\n",
        "\n",
        "      # neu khong nham trong bang chu cai thi bo qua\n",
        "      if not re.match(alphabet, p.lower()):\n",
        "        continue\n",
        "\n",
        "      # tach p thanh cac bi gram\n",
        "      for ngr in gen_ngrams(p, NGRAM):\n",
        "        if len(\" \".join(ngr)) < MAXLEN:\n",
        "          list_ngrams.append(\" \".join(ngr))\n",
        "\n",
        "    return list_ngrams\n",
        "\n",
        "list_ngrams = generate_bi_grams(phrases)\n",
        "\n",
        "print(len(list_ngrams))\n",
        "\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "\n",
        "#Chuyển đổi một đoạn văn bản thành dạng mã hóa one-hot có độ dài cố định (MAXLEN).\n",
        "#Đầu ra: Mảng 2D one-hot encoding của văn bản\n",
        "def encoder_data(text, maxlen=MAXLEN):\n",
        "        #print(\"Maxlen\", maxlen)\n",
        "        text = \"\\x00\" + text\n",
        "        #print(\"text\", text)\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        #print(\"X ban dau\", x)\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "#Mục đích: Giải mã dữ liệu one-hot encoding trở lại thành văn bản.\n",
        "#Đầu ra: Văn bản đã được giải mã.\n",
        "def decoder_data(x):\n",
        "    x = x.argmax(axis=-1)\n",
        "    #print(\"x hien tai\", x)\n",
        "    dem = ''.join(alphabet[i] for i in x)\n",
        "    #print(\"Do dai cau van\", len(dem))\n",
        "\n",
        "    return dem\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGu5z5Q3GeCu",
        "outputId": "2c8ee5a6-4f44-48c3-f43c-efab4212ed51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64294/64294 [00:00<00:00, 109226.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "614698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import preprocessing\n",
        "import os\n",
        "# from preprocessing import MAXLEN, alphabet\n",
        "\n",
        "encoder = LSTM(256,input_shape=(MAXLEN, len(alphabet)), return_sequences=True)\n",
        "\n",
        "decoder = Bidirectional(LSTM(256, return_sequences=True, dropout=0.2))\n",
        "\n",
        "model=Sequential()\n",
        "model.add(encoder)\n",
        "model.add(decoder)\n",
        "model.add(TimeDistributed(Dense(256)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(TimeDistributed(Dense(len(alphabet))))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#Spliting training data\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)\n",
        "\n",
        "# we have to use data- generation medthod cause this dataset is too large to fit into memory\n",
        "BATCH_SIZE = 512\n",
        "def generate_data(data, batch_size):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):\n",
        "            y.append(encoder_data(data[cur_index]))\n",
        "            x.append(encoder_data(add_noise(data[cur_index],0.94,0.985)))\n",
        "            cur_index += 1\n",
        "            if cur_index > len(data)-1:\n",
        "                cur_index = 0\n",
        "        yield np.array(x), np.array(y)\n",
        "\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# train the model and save to the Model folder\n",
        "checkpointer = ModelCheckpoint(filepath=os.path.join('/content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5'), save_best_only=True, verbose=1)\n",
        "\n",
        "model.fit( train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=5,\n",
        "                    validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "                    callbacks=[checkpointer] )"
      ],
      "metadata": {
        "id": "QRaZ0FE2HLmc",
        "outputId": "ffc6b2fd-bfe1-4bb5-d937-4a7185f77d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 40, 256)           466944    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 40, 512)           1050624   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 40, 256)           131328    \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " activation (Activation)     (None, 40, 256)           0         \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 40, 199)           51143     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 40, 199)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1700039 (6.49 MB)\n",
            "Trainable params: 1700039 (6.49 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "960/960 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.9271\n",
            "Epoch 1: val_loss improved from inf to 0.07287, saving model to /content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r960/960 [==============================] - 160s 148ms/step - loss: 0.3517 - accuracy: 0.9271 - val_loss: 0.0729 - val_accuracy: 0.9855\n",
            "Epoch 2/5\n",
            "960/960 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9874\n",
            "Epoch 2: val_loss improved from 0.07287 to 0.04660, saving model to /content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5\n",
            "960/960 [==============================] - 152s 159ms/step - loss: 0.0595 - accuracy: 0.9874 - val_loss: 0.0466 - val_accuracy: 0.9895\n",
            "Epoch 3/5\n",
            "960/960 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9906\n",
            "Epoch 3: val_loss improved from 0.04660 to 0.03575, saving model to /content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5\n",
            "960/960 [==============================] - 137s 143ms/step - loss: 0.0416 - accuracy: 0.9906 - val_loss: 0.0358 - val_accuracy: 0.9917\n",
            "Epoch 4/5\n",
            "960/960 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9921\n",
            "Epoch 4: val_loss improved from 0.03575 to 0.02944, saving model to /content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5\n",
            "960/960 [==============================] - 136s 142ms/step - loss: 0.0340 - accuracy: 0.9921 - val_loss: 0.0294 - val_accuracy: 0.9931\n",
            "Epoch 5/5\n",
            "960/960 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9931\n",
            "Epoch 5: val_loss improved from 0.02944 to 0.02747, saving model to /content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5\n",
            "960/960 [==============================] - 153s 160ms/step - loss: 0.0293 - accuracy: 0.9931 - val_loss: 0.0275 - val_accuracy: 0.9936\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x796c1dfbf310>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(add_noise(\"chúc mừng năm mới 2024\",0.94,0.985));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISshVtJZRb4G",
        "outputId": "7690b56c-50ee-4f67-8f64-d5453399c969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chúc mừng năm mới 2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from nltk import ngrams,word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5')\n",
        "\n",
        "NGRAM=2\n",
        "MAXLEN=40\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "accepted_char=list((string.digits + ''.join(letters)))\n",
        "\n",
        "def call(sentence):\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "            text = \"\\x00\" + text\n",
        "            x = np.zeros((maxlen, len(alphabet)))\n",
        "            for i, c in enumerate(text[:maxlen]):\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "            if i < maxlen - 1:\n",
        "              for j in range(i+1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "            return x\n",
        "\n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def guess(ngram):\n",
        "        text = ' '.join(ngram)\n",
        "        preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n",
        "        return decoder_data(preds[0]).strip('\\x00')\n",
        "\n",
        "    def correct(sentence):\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence=sentence.replace(i,\" \")\n",
        "        ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n",
        "\n",
        "        print(\"N gram\", ngrams)\n",
        "        print(\"guess\", guessed_ngrams)\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in (enumerate(guessed_ngrams)):\n",
        "            for wid, word in (enumerate(re.split(' +', ngram))):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates)\n",
        "        return output\n",
        "\n",
        "    guess = correct(sentence)\n",
        "\n",
        "    return guess\n",
        "\n",
        "sentence = \"Tôi là sinh vien trường đại họ cPhenikaa taji Hà Dông\"\n",
        "# sentence = 'Tôi phông iu thích hok ngôn ngữ mới'\n",
        "# sentence = 'tôi teen laf Tô Văn Tus, toi ddang hojc lớp 5'\n",
        "# sentence = \"chusc mừg năm mới 2024\"\n",
        "\n",
        "guess = call(sentence)\n",
        "print(guess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6VW5NmvHW_F",
        "outputId": "0cf8a714-fe53-4c08-9e9f-2c481afc49eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N gram [('Tôi', 'là'), ('là', 'sinh'), ('sinh', 'vien'), ('vien', 'trường'), ('trường', 'đại'), ('đại', 'họ'), ('họ', 'cPhenikaa'), ('cPhenikaa', 'taji'), ('taji', 'Hà'), ('Hà', 'Dông')]\n",
            "guess ['Tôi là', 'là sinh', 'sinh viện', 'viện trường', 'trường đại', 'đại họ', 'họ cPhenikaa', 'cPhenikaa tại', 'tại Hà', 'Hà Dông']\n",
            "Tôi là sinh viện trường đại họ cPhenikaa tại Hà Dông\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mg5g_hbeKPwa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}